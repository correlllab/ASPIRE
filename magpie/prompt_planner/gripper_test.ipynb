{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test Notebook for Gripper class functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from magpie.gripper import Gripper\n",
    "import time\n",
    "servoport = '/dev/ttyACM0'\n",
    "G = Gripper(servoport=servoport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the gripper to set aperture\n",
    "theta = G.aperture_to_theta(10) # 10mm aperture between finger and camera x-center\n",
    "G.Finger1 = G.set_goal_position(G.theta_to_position(theta, finger='left'))\n",
    "G.Finger2 = G.set_goal_position(G.theta_to_position(theta, finger='right'))\n",
    "# validate z-offset (10mm aperture on either finger should be 20mm)\n",
    "print(f\"z-offset: {G.aperture_to_z(20, finger='both')} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the same distance\n",
    "G.set_goal_aperture(20, finger='both') # 10mm aperture between each finger and camera x-center, totalling 20mm\n",
    "# validate z-offset\n",
    "print(f\"z-offset: {G.aperture_to_z(20, finger='both')} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Dylan's code\n",
    "G.set_goal_distance(20) # 10mm aperture between each finger and camera x-center, totalling 20mm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Getters and Setters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.get_load(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.get_position(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.get_temperature(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.get_aperture(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with Dylan's code\n",
    "print(G.get_distance(finger='both'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 N\n",
    "G.set_force(1.0, finger='both', debug=True)\n",
    "time.sleep(0.005)\n",
    "G.get_load(finger='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set distance to 20 mm \n",
    "G.set_goal_distance(20, debug=True)\n",
    "time.sleep(0.5)\n",
    "print(G.get_distance(finger='both'))\n",
    "print(G.get_aperture(finger='both'))\n",
    "print(G.get_position(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set compliance (in mm, slope[0-7])\n",
    "G.set_compliance(5, 4, finger='both', debug=True)\n",
    "time.sleep(0.005) \n",
    "print(G.get_compliance(finger='both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/docs/guides/vision\n",
    "import openai\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "import torch\n",
    "from transformers.image_utils import ImageFeatureExtractionMixin\n",
    "\n",
    "# import open3d as o3d\n",
    "import numpy as np\n",
    "# import pyrealsense2 as rs\n",
    "import matplotlib.pyplot as plt\n",
    "# import RealSense as real\n",
    "\n",
    "# # Initialize RS435i connection\n",
    "# rsc = real.RealSense()\n",
    "# rsc.initConnection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8q2WxRjlJrltz2XHTlAbPBmXc1HHz', 'object': 'chat.completion', 'created': 1707413475, 'model': 'gpt-4-1106-vision-preview', 'usage': {'prompt_tokens': 2058, 'completion_tokens': 169, 'total_tokens': 2227}, 'choices': [{'message': {'role': 'assistant', 'content': 'Based on the provided images, the following errors in the labels have been identified:\\n\\n1. The object labeled as \"orange (4)\" is actually a red apple.\\n2. The item labeled as \"orange (5)\" is a red apple.\\n3. The item labeled as \"orange (9)\" is a green pear.\\n\\nGiven this information, the corrected list of labels with matching indices is:\\n\\n- orange (0)\\n- orange (1)\\n- orange (2)\\n- apple (3)\\n- apple (4)\\n- orange (6)\\n- orange (7)\\n- orange (8)\\n- pear (9)\\n- orange (10)\\n- orange (11)\\n- orange (12)\\n- orange (13)\\n- orange (14)\\n- orange (15)\\n- orange (16)\\n- orange (17)\\n- orange (18)'}, 'finish_reason': 'stop', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "# Path to your image\n",
    "image_path = \"data_snapshots/rgb_image_ur5.png\"\n",
    "image_path_labeled = 'data_snapshots/owlvit_inference.png'\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "base64_image_labeled = encode_image(image_path_labeled)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "text1 = \"\"\"\n",
    "You are planning a grasp for a robotic manipulator in a grocery bin. \n",
    "You will validate whether the labeled objects are present in the bin and correct the list of labels if there are any incongruities.\n",
    "For example if a the label in the image is \"apple (3)\" but the object is a \"pear\", you will correct the label to \"pear (3)\" in the format of the provided caption.\n",
    "The number in the image label corresponds to the index of the object in the list of objects.\n",
    "You are provided with a photo of the grocery bin, a photo of the indexed and labeled grocery bins, and a list of labels with matching indices.\n",
    "In your response, first identify any errors in the labels or missing labels. Then, finish your response in the format of the provided caption, with corrections applied.\n",
    "Make sure to mention the index of the fruit associated with the error.\n",
    "\"\"\"\n",
    "\n",
    "resp1 = \"\"\"\n",
    "I see: [\"a photo of a lemon\", \"a photo of an apple\", \"a photo of a pear\", \"a photo of an onion\", \"a photo of a lime\"]\n",
    "Labels: [\"lemon\", \"apple\", \"pear\", \"onion\", \"lime\"]\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Please generate a list of labels for this new image of the grocery bin, in the same format.\n",
    "Are there any new fruits or vegetables?\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "No further information required.\n",
    "Now I am looking at a painting of people looking at a shore.\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{text1}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image_labeled}\"\n",
    "            }\n",
    "          }]\n",
    "      },\n",
    "      # {\n",
    "      #   \"role\": \"assistant\",\n",
    "      #   \"content\": [\n",
    "      #     {\n",
    "      #       \"type\": \"text\",\n",
    "      #       \"text\": f\"{resp1}\"\n",
    "      #     }]\n",
    "      # },\n",
    "      # {\n",
    "      #   \"role\": \"user\",\n",
    "      #   \"content\": [\n",
    "      #     {\n",
    "      #       \"type\": \"text\",\n",
    "      #       \"text\": f\"{text2}\"\n",
    "      #     },\n",
    "      #     {\n",
    "      #       \"type\": \"image_url\",\n",
    "      #       \"image_url\": {\n",
    "      #         \"url\": f\"data:image/jpeg;base64,{base64_image_avo}\"\n",
    "      #       }\n",
    "      #     }]\n",
    "      # }          \n",
    "      ],\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided images, the following errors in the labels have been identified:\\n\\n1. The object labeled as \"orange (4)\" is actually a red apple.\\n2. The item labeled as \"orange (5)\" is a red apple.\\n3. The item labeled as \"orange (9)\" is a green pear.\\n\\nGiven this information, the corrected list of labels with matching indices is:\\n\\n- orange (0)\\n- orange (1)\\n- orange (2)\\n- apple (3)\\n- apple (4)\\n- orange (6)\\n- orange (7)\\n- orange (8)\\n- pear (9)\\n- orange (10)\\n- orange (11)\\n- orange (12)\\n- orange (13)\\n- orange (14)\\n- orange (15)\\n- orange (16)\\n- orange (17)\\n- orange (18)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = response.json()[\"choices\"][0]['message']['content']\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = \"\"\"\n",
    "Here is an example of a correct response to these images:\n",
    "Based on the provided images, the following errors in the labels have been identified:\n",
    "n1. The object labeled as \"orange (0)\" in the top right is actually a yellow lemon.\n",
    "n2. The item labeled as \"orange (3)\" to the left of \"orange (0)\" is a yellow lemon.\n",
    "n3. The item labeled as \"orange (6)\" in the far right is a yellow lemon.\n",
    "n2. The item labeled as \"orange (8)\" to the left of \"orange (6)\" is a yellow lemon.\n",
    "Given this information, the corrected list of labels with matching indices is:\\n\\n- lemon (0)\\n- orange (1)\\n- orange (2)\\n- lemon (3)\\n- orange (4)\\n- orange (5)\\n- lemon (6)\\n- orange (7)\\n- lemon (8)\\n- orange (9)\\n- orange (10)\\n- orange (11)\\n- orange (12)\\n- orange (13)\\n- orange (14)\\n- orange (15)\\n- orange (16)\\n- orange (17)\\n- orange (18)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "You are planning a grasp for a robotic manipulator in a grocery bin. \n",
    "You will validate whether the labeled objects are present in the bin and correct the list of labels if there are any incongruities.\n",
    "For example if a the label in the image is \"apple (3)\" but the object is a \"pear\", you will correct the label to \"pear (3)\" in the format of the provided caption.\n",
    "The number in the image label corresponds to the index of the object in the list of objects.\n",
    "You are provided with a photo of the grocery bin, a photo of the indexed and labeled grocery bins, and a list of labels with matching indices.\n",
    "In your response, first identify any errors in the labels or missing labels. Then, finish your response in the format of the provided caption, with corrections applied.\n",
    "Make sure to mention the index of the fruit associated with the error.\n",
    "\n",
    "Here is an example of a caption:\n",
    "\"\"\"\n",
    "text1 = text1 + context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now, please identifiy any errors or missing labels in the next image. Provided caption: \\npear (0): ([1003.6107110977173, 109.06752079725266, 1086.157660484314, 173.9412572979927], 'pear')\\npear (1): ([823.7149906158447, 91.88447713851929, 982.2424983978271, 239.59284782409668], 'pear')\\npear (2): ([573.7592506408691, 218.29054534435272, 660.1446342468262, 311.40706837177277], 'pear')\\npear (3): ([620.7485294342041, 263.33383083343506, 804.0051937103271, 411.74015522003174], 'pear')\\npear (4): ([953.1515741348267, 304.79566991329193, 1039.001441001892, 410.0387817621231], 'pear')\\npear (5): ([624.2333507537842, 260.7258278131485, 803.745927810669, 409.4582122564316], 'pear')\\npear (6): ([948.1863212585449, 302.46907353401184, 1031.769733428955, 430.90051531791687], 'pear')\\npear (7): ([501.7671251296997, 371.20080292224884, 600.0220441818237, 477.94571578502655], 'pear')\\npear (8): ([579.2618751525879, 393.62665593624115, 659.7358512878418, 501.1235100030899], 'pear')\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"\"\"Now, please identifiy any errors or missing labels in the next image. Provided caption: \"\"\"\n",
    "text2 = text2 + context_pear\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8q2phOt1556w4tx7WqU92SYBQLnYW', 'object': 'chat.completion', 'created': 1707414637, 'model': 'gpt-4-1106-vision-preview', 'usage': {'prompt_tokens': 3922, 'completion_tokens': 104, 'total_tokens': 4026}, 'choices': [{'message': {'role': 'assistant', 'content': 'Based on the provided image, here is a list of labels with corrections to the indices and types of fruits they correspond to:\\n\\n- pear (0)\\n- pear (1)\\n- pear (2)\\n- pear (3)\\n- pear (4)\\n- pear (5)\\n- pear (6)\\n- pear (7)\\n- pear (8)\\n\\nThere are no new fruits or vegetables in the image beyond the pears. Each labeled item appears to be a pear, correctly matching the label. No adjustments are needed.'}, 'finish_reason': 'stop', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "# Path to your image\n",
    "image_path = \"data_snapshots/rgb_image_ur5.png\"\n",
    "image_path_orange = 'data_snapshots/owlvit_inference-orange.png'\n",
    "image_path_pear = 'data_snapshots/owlvit_inference-pear.png'\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "base64_image_orange = encode_image(image_path_orange)\n",
    "base64_image_pear = encode_image(image_path_pear)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# resp1 = \"\"\"\n",
    "# I see: [\"a photo of a lemon\", \"a photo of an apple\", \"a photo of a pear\", \"a photo of an onion\", \"a photo of a lime\"]\n",
    "# Labels: [\"lemon\", \"apple\", \"pear\", \"onion\", \"lime\"]\n",
    "# \"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Please generate a list of labels for this new image of the grocery bin, in the same format.\n",
    "Are there any new fruits or vegetables?\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "No further information required.\n",
    "Now I am looking at a painting of people looking at a shore.\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{text1}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image_orange}\"\n",
    "            }\n",
    "          }\n",
    "          ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{resp1}\"\n",
    "          }]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{text2}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          }\n",
    "          ,\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image_pear}\"\n",
    "            }\n",
    "          }\n",
    "          ]\n",
    "      }          \n",
    "      ],\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided image, here is a list of labels with corrections to the indices and types of fruits they correspond to:\\n\\n- pear (0)\\n- pear (1)\\n- pear (2)\\n- pear (3)\\n- pear (4)\\n- pear (5)\\n- pear (6)\\n- pear (7)\\n- pear (8)\\n\\nThere are no new fruits or vegetables in the image beyond the pears. Each labeled item appears to be a pear, correctly matching the label. No adjustments are needed.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = response.json()[\"choices\"][0]['message']['content']\n",
    "generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
