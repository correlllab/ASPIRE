{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test Notebook for Gripper class functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'magpie.gripper' from '/home/will/MAGPIE/magpie/prompt_planner/../../magpie/gripper.py'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded to open the port\n",
      "Succeeded to change the baudrate\n",
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import magpie.gripper as g\n",
    "from magpie.gripper import Gripper\n",
    "import time\n",
    "servoport = '/dev/ttyACM1'\n",
    "G = Gripper(servoport=servoport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torque enable dxl ID: 1 set to 1 \n",
      "[RxPacketError] Overload error!\n",
      "[RxPacketError] Overload error!\n",
      "Torque enable dxl ID: 2 set to 1 \n",
      "[RxPacketError] Overload error!\n"
     ]
    }
   ],
   "source": [
    "G.reset_packet_overload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 1 set to 586 \n",
      "Position of dxl ID: 2 set to 450 \n"
     ]
    }
   ],
   "source": [
    "G.close_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving speed of dxl ID: 1 set to 100 \n",
      "Moving speed of dxl ID: 2 set to 100 \n",
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n"
     ]
    }
   ],
   "source": [
    "G.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n"
     ]
    }
   ],
   "source": [
    "G.open_gripper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.40480967920502"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.get_aperture(finger='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving speed of dxl ID: 1 set to 60 \n",
      "Moving speed of dxl ID: 2 set to 60 \n"
     ]
    }
   ],
   "source": [
    "G.set_torque(130, finger='both')\n",
    "G.set_speed(60, finger='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 308\n",
      "Position of dxl ID: 1 set to 308 \n",
      "position: 731\n",
      "Position of dxl ID: 2 set to 731 \n"
     ]
    }
   ],
   "source": [
    "desired_dist = 105\n",
    "G.set_goal_aperture(desired_dist, finger='both', debug=True) #25mm aperture between fingers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: 53.8146318475935, l2: 26.567980728689193, l1: 81.32, y_fingertip: 86.88798072868919\n",
      "z-offset: 86.88798072868919 mm\n"
     ]
    }
   ],
   "source": [
    "# validate z-offset (10mm aperture on either finger should be 20mm)\n",
    "zoff = G.aperture_to_z(desired_dist, finger='both', debug=True)\n",
    "print(f\"z-offset: {zoff} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.48645436150926"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.get_aperture(finger='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.Finger1.get_present_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.9484487042254"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.get_aperture(finger='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 438\n",
      "Position of dxl ID: 1 set to 295 \n",
      "Position of dxl ID: 1 set to 294 \n",
      "Position of dxl ID: 1 set to 293 \n",
      "Position of dxl ID: 1 set to 290 \n",
      "Position of dxl ID: 1 set to 289 \n",
      "Position of dxl ID: 1 set to 288 \n",
      "Position of dxl ID: 1 set to 287 \n",
      "Position of dxl ID: 1 set to 286 \n",
      "Position of dxl ID: 1 set to 285 \n",
      "Position of dxl ID: 1 set to 284 \n",
      "Position of dxl ID: 1 set to 283 \n",
      "Position of dxl ID: 1 set to 282 \n",
      "Position of dxl ID: 1 set to 281 \n",
      "Position of dxl ID: 1 set to 280 \n",
      "Position of dxl ID: 1 set to 279 \n",
      "Position of dxl ID: 1 set to 278 \n",
      "Position of dxl ID: 1 set to 277 \n",
      "Position of dxl ID: 1 set to 275 \n",
      "Position of dxl ID: 1 set to 274 \n",
      "Position of dxl ID: 1 set to 273 \n",
      "Position of dxl ID: 1 set to 272 \n",
      "left finger reached stop force: 3.2689470400000005 at distance: 54.252107976649704\n",
      "left finger reached stop load: 192 at position: 292\n",
      "Position of dxl ID: 1 set to 292 \n"
     ]
    }
   ],
   "source": [
    "G.close_until_contact_force(30, 3.0, finger='left', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.get_load(finger='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "margin_ax12: 9\n",
      "flexibility_ax12: 128\n",
      "margin_ax12: 9\n",
      "flexibility_ax12: 128\n"
     ]
    }
   ],
   "source": [
    "G.set_compliance(2, 7, finger='left', debug=True)\n",
    "G.set_compliance(2, 7, finger='right', debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.20311759145189, 52.04459477753785]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[G.get_aperture(finger='left'), G.get_aperture(finger='right')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: 601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(601, 100.18198855127346)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "finger = 'right'\n",
    "finger_ax12 = G.Finger1 if finger == 'left' else G.Finger2\n",
    "latency = 0.0006\n",
    "delay = 0.0055\n",
    "stop_force = 0.25\n",
    "stop_load = G.N_to_load(stop_force)\n",
    "stop_aperture = 30 # mm\n",
    "curr_pos = G.get_aperture(finger=finger)\n",
    "curr_pos = finger_ax12.get_present_position()\n",
    "stop_ax12 = G.theta_to_position(G.aperture_to_theta(stop_aperture), finger=finger, debug=True)\n",
    "delta = stop_ax12 - int(curr_pos)\n",
    "sign = np.sign(delta)\n",
    "\n",
    "stop_ax12, stop_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.set_torque(120, finger='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 2 set to 728 \n"
     ]
    }
   ],
   "source": [
    "finger_ax12.set_goal_position(728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(728, 601)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_pos = finger_ax12.get_present_position()\n",
    "range(curr_pos, stop_ax12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 2 set to 642 \n",
      "curr_load: 0, curr_pos: 728\n",
      "Position of dxl ID: 2 set to 641 \n",
      "curr_load: 0, curr_pos: 728\n",
      "Position of dxl ID: 2 set to 640 \n",
      "curr_load: 0, curr_pos: 726\n",
      "Position of dxl ID: 2 set to 639 \n",
      "curr_load: 0, curr_pos: 725\n",
      "Position of dxl ID: 2 set to 638 \n",
      "curr_load: 88, curr_pos: 723\n",
      "Position of dxl ID: 2 set to 637 \n",
      "curr_load: 88, curr_pos: 722\n",
      "Position of dxl ID: 2 set to 636 \n",
      "curr_load: 88, curr_pos: 719\n",
      "Position of dxl ID: 2 set to 635 \n",
      "curr_load: 88, curr_pos: 717\n",
      "Position of dxl ID: 2 set to 634 \n",
      "curr_load: 88, curr_pos: 715\n",
      "Position of dxl ID: 2 set to 633 \n",
      "curr_load: 88, curr_pos: 712\n",
      "Position of dxl ID: 2 set to 632 \n",
      "curr_load: 88, curr_pos: 710\n",
      "Position of dxl ID: 2 set to 631 \n",
      "curr_load: 16, curr_pos: 708\n",
      "Position of dxl ID: 2 set to 630 \n",
      "curr_load: 16, curr_pos: 706\n",
      "Position of dxl ID: 2 set to 629 \n",
      "curr_load: 16, curr_pos: 704\n",
      "Position of dxl ID: 2 set to 628 \n",
      "curr_load: 16, curr_pos: 700\n",
      "Position of dxl ID: 2 set to 627 \n",
      "curr_load: 16, curr_pos: 699\n",
      "Position of dxl ID: 2 set to 626 \n",
      "curr_load: 16, curr_pos: 697\n",
      "Position of dxl ID: 2 set to 625 \n",
      "curr_load: 16, curr_pos: 695\n",
      "Position of dxl ID: 2 set to 624 \n",
      "curr_load: 8, curr_pos: 693\n",
      "Position of dxl ID: 2 set to 623 \n",
      "curr_load: 8, curr_pos: 691\n",
      "Position of dxl ID: 2 set to 622 \n",
      "curr_load: 8, curr_pos: 689\n",
      "Position of dxl ID: 2 set to 621 \n",
      "curr_load: 8, curr_pos: 687\n",
      "Position of dxl ID: 2 set to 620 \n",
      "curr_load: 8, curr_pos: 684\n",
      "Position of dxl ID: 2 set to 619 \n",
      "curr_load: 8, curr_pos: 682\n",
      "Position of dxl ID: 2 set to 618 \n",
      "curr_load: 8, curr_pos: 680\n",
      "Position of dxl ID: 2 set to 617 \n",
      "curr_load: 8, curr_pos: 678\n",
      "Position of dxl ID: 2 set to 616 \n",
      "curr_load: 8, curr_pos: 674\n",
      "Position of dxl ID: 2 set to 615 \n",
      "curr_load: 8, curr_pos: 672\n",
      "Position of dxl ID: 2 set to 614 \n",
      "curr_load: 8, curr_pos: 670\n",
      "Position of dxl ID: 2 set to 613 \n",
      "curr_load: 8, curr_pos: 668\n",
      "Position of dxl ID: 2 set to 612 \n",
      "curr_load: 8, curr_pos: 666\n",
      "Position of dxl ID: 2 set to 611 \n",
      "curr_load: 8, curr_pos: 664\n",
      "Position of dxl ID: 2 set to 610 \n",
      "curr_load: 8, curr_pos: 660\n",
      "Position of dxl ID: 2 set to 609 \n",
      "curr_load: 8, curr_pos: 658\n",
      "Position of dxl ID: 2 set to 608 \n",
      "curr_load: 8, curr_pos: 656\n",
      "Position of dxl ID: 2 set to 607 \n",
      "curr_load: 8, curr_pos: 654\n",
      "Position of dxl ID: 2 set to 606 \n",
      "curr_load: 8, curr_pos: 651\n",
      "Position of dxl ID: 2 set to 605 \n",
      "curr_load: 8, curr_pos: 650\n",
      "Position of dxl ID: 2 set to 604 \n",
      "curr_load: 8, curr_pos: 647\n",
      "Position of dxl ID: 2 set to 603 \n",
      "curr_load: 8, curr_pos: 645\n",
      "Position of dxl ID: 2 set to 602 \n",
      "curr_load: 8, curr_pos: 644\n"
     ]
    }
   ],
   "source": [
    "time.sleep(latency)\n",
    "for next_pos in range(curr_pos, stop_ax12, sign*1):\n",
    "    finger_ax12.set_goal_position(next_pos)\n",
    "    time.sleep(delay)\n",
    "    time.sleep(delay)\n",
    "    curr_pos = finger_ax12.get_present_position()\n",
    "    curr_load = finger_ax12.get_load()\n",
    "    if curr_load > 1023:\n",
    "        curr_load = curr_load - 1023\n",
    "    print(f'curr_load: {curr_load}, curr_pos: {curr_pos}')\n",
    "    time.sleep(delay)\n",
    "    time.sleep(latency)\n",
    "    if curr_load > stop_load:\n",
    "        force = G.load_to_N(curr_load)\n",
    "        distance = G.get_aperture(finger=finger)\n",
    "        print(f'{finger} finger reached stop force: {force} at distance: {distance}')\n",
    "        print(f'{finger} finger reached stop load: {curr_load} at position: {curr_pos}')\n",
    "        # todo: set goal_distance_fx class variable, but not really necessary\n",
    "        finger_ax12.set_goal_position(curr_pos)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 2 set to 728 \n"
     ]
    }
   ],
   "source": [
    "finger_ax12.set_goal_position(728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.get_load(finger='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of dxl ID: 1 set to 303 \n",
      "Position of dxl ID: 2 set to 729 \n"
     ]
    }
   ],
   "source": [
    "G.open_gripper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Getters and Setters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(G.get_load(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[526, 511]\n"
     ]
    }
   ],
   "source": [
    "print(G.get_position(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 36]\n"
     ]
    }
   ],
   "source": [
    "print(G.get_temp(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.986677024361736\n"
     ]
    }
   ],
   "source": [
    "print(G.get_aperture(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.138556173746146\n"
     ]
    }
   ],
   "source": [
    "# compare with Dylan's code\n",
    "print(G.get_distance(finger='both'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09888000000000001"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.load_to_N(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.45832530419774"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.N_to_load(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted load: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 N\n",
    "G.set_force(2.0, finger='both', debug=True)\n",
    "time.sleep(0.005)\n",
    "G.get_load(finger='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set distance to 20 mm \n",
    "G.set_goal_distance(20, debug=True)\n",
    "time.sleep(0.5)\n",
    "print(G.get_distance(finger='both'))\n",
    "print(G.get_aperture(finger='both'))\n",
    "print(G.get_position(finger='both'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set compliance (in mm, slope[0-7])\n",
    "G.set_compliance(5, 4, finger='both', debug=True)\n",
    "time.sleep(0.005) \n",
    "print(G.get_compliance(finger='both'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/docs/guides/vision\n",
    "import openai\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "import torch\n",
    "from transformers.image_utils import ImageFeatureExtractionMixin\n",
    "\n",
    "# import open3d as o3d\n",
    "import numpy as np\n",
    "# import pyrealsense2 as rs\n",
    "import matplotlib.pyplot as plt\n",
    "# import RealSense as real\n",
    "\n",
    "# # Initialize RS435i connection\n",
    "# rsc = real.RealSense()\n",
    "# rsc.initConnection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8q2WxRjlJrltz2XHTlAbPBmXc1HHz', 'object': 'chat.completion', 'created': 1707413475, 'model': 'gpt-4-1106-vision-preview', 'usage': {'prompt_tokens': 2058, 'completion_tokens': 169, 'total_tokens': 2227}, 'choices': [{'message': {'role': 'assistant', 'content': 'Based on the provided images, the following errors in the labels have been identified:\\n\\n1. The object labeled as \"orange (4)\" is actually a red apple.\\n2. The item labeled as \"orange (5)\" is a red apple.\\n3. The item labeled as \"orange (9)\" is a green pear.\\n\\nGiven this information, the corrected list of labels with matching indices is:\\n\\n- orange (0)\\n- orange (1)\\n- orange (2)\\n- apple (3)\\n- apple (4)\\n- orange (6)\\n- orange (7)\\n- orange (8)\\n- pear (9)\\n- orange (10)\\n- orange (11)\\n- orange (12)\\n- orange (13)\\n- orange (14)\\n- orange (15)\\n- orange (16)\\n- orange (17)\\n- orange (18)'}, 'finish_reason': 'stop', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "# Path to your image\n",
    "image_path = \"data_snapshots/rgb_image_ur5.png\"\n",
    "image_path_labeled = 'data_snapshots/owlvit_inference.png'\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "base64_image_labeled = encode_image(image_path_labeled)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "text1 = \"\"\"\n",
    "You are planning a grasp for a robotic manipulator in a grocery bin. \n",
    "You will validate whether the labeled objects are present in the bin and correct the list of labels if there are any incongruities.\n",
    "For example if a the label in the image is \"apple (3)\" but the object is a \"pear\", you will correct the label to \"pear (3)\" in the format of the provided caption.\n",
    "The number in the image label corresponds to the index of the object in the list of objects.\n",
    "You are provided with a photo of the grocery bin, a photo of the indexed and labeled grocery bins, and a list of labels with matching indices.\n",
    "In your response, first identify any errors in the labels or missing labels. Then, finish your response in the format of the provided caption, with corrections applied.\n",
    "Make sure to mention the index of the fruit associated with the error.\n",
    "\"\"\"\n",
    "\n",
    "resp1 = \"\"\"\n",
    "I see: [\"a photo of a lemon\", \"a photo of an apple\", \"a photo of a pear\", \"a photo of an onion\", \"a photo of a lime\"]\n",
    "Labels: [\"lemon\", \"apple\", \"pear\", \"onion\", \"lime\"]\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Please generate a list of labels for this new image of the grocery bin, in the same format.\n",
    "Are there any new fruits or vegetables?\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "No further information required.\n",
    "Now I am looking at a painting of people looking at a shore.\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{text1}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image_labeled}\"\n",
    "            }\n",
    "          }]\n",
    "      },\n",
    "      # {\n",
    "      #   \"role\": \"assistant\",\n",
    "      #   \"content\": [\n",
    "      #     {\n",
    "      #       \"type\": \"text\",\n",
    "      #       \"text\": f\"{resp1}\"\n",
    "      #     }]\n",
    "      # },\n",
    "      # {\n",
    "      #   \"role\": \"user\",\n",
    "      #   \"content\": [\n",
    "      #     {\n",
    "      #       \"type\": \"text\",\n",
    "      #       \"text\": f\"{text2}\"\n",
    "      #     },\n",
    "      #     {\n",
    "      #       \"type\": \"image_url\",\n",
    "      #       \"image_url\": {\n",
    "      #         \"url\": f\"data:image/jpeg;base64,{base64_image_avo}\"\n",
    "      #       }\n",
    "      #     }]\n",
    "      # }          \n",
    "      ],\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided images, the following errors in the labels have been identified:\\n\\n1. The object labeled as \"orange (4)\" is actually a red apple.\\n2. The item labeled as \"orange (5)\" is a red apple.\\n3. The item labeled as \"orange (9)\" is a green pear.\\n\\nGiven this information, the corrected list of labels with matching indices is:\\n\\n- orange (0)\\n- orange (1)\\n- orange (2)\\n- apple (3)\\n- apple (4)\\n- orange (6)\\n- orange (7)\\n- orange (8)\\n- pear (9)\\n- orange (10)\\n- orange (11)\\n- orange (12)\\n- orange (13)\\n- orange (14)\\n- orange (15)\\n- orange (16)\\n- orange (17)\\n- orange (18)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = response.json()[\"choices\"][0]['message']['content']\n",
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = \"\"\"\n",
    "Here is an example of a correct response to these images:\n",
    "Based on the provided images, the following errors in the labels have been identified:\n",
    "n1. The object labeled as \"orange (0)\" in the top right is actually a yellow lemon.\n",
    "n2. The item labeled as \"orange (3)\" to the left of \"orange (0)\" is a yellow lemon.\n",
    "n3. The item labeled as \"orange (6)\" in the far right is a yellow lemon.\n",
    "n2. The item labeled as \"orange (8)\" to the left of \"orange (6)\" is a yellow lemon.\n",
    "Given this information, the corrected list of labels with matching indices is:\\n\\n- lemon (0)\\n- orange (1)\\n- orange (2)\\n- lemon (3)\\n- orange (4)\\n- orange (5)\\n- lemon (6)\\n- orange (7)\\n- lemon (8)\\n- orange (9)\\n- orange (10)\\n- orange (11)\\n- orange (12)\\n- orange (13)\\n- orange (14)\\n- orange (15)\\n- orange (16)\\n- orange (17)\\n- orange (18)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "You are planning a grasp for a robotic manipulator in a grocery bin. \n",
    "You will validate whether the labeled objects are present in the bin and correct the list of labels if there are any incongruities.\n",
    "For example if a the label in the image is \"apple (3)\" but the object is a \"pear\", you will correct the label to \"pear (3)\" in the format of the provided caption.\n",
    "The number in the image label corresponds to the index of the object in the list of objects.\n",
    "You are provided with a photo of the grocery bin, a photo of the indexed and labeled grocery bins, and a list of labels with matching indices.\n",
    "In your response, first identify any errors in the labels or missing labels. Then, finish your response in the format of the provided caption, with corrections applied.\n",
    "Make sure to mention the index of the fruit associated with the error.\n",
    "\n",
    "Here is an example of a caption:\n",
    "\"\"\"\n",
    "text1 = text1 + context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Now, please identifiy any errors or missing labels in the next image. Provided caption: \\npear (0): ([1003.6107110977173, 109.06752079725266, 1086.157660484314, 173.9412572979927], 'pear')\\npear (1): ([823.7149906158447, 91.88447713851929, 982.2424983978271, 239.59284782409668], 'pear')\\npear (2): ([573.7592506408691, 218.29054534435272, 660.1446342468262, 311.40706837177277], 'pear')\\npear (3): ([620.7485294342041, 263.33383083343506, 804.0051937103271, 411.74015522003174], 'pear')\\npear (4): ([953.1515741348267, 304.79566991329193, 1039.001441001892, 410.0387817621231], 'pear')\\npear (5): ([624.2333507537842, 260.7258278131485, 803.745927810669, 409.4582122564316], 'pear')\\npear (6): ([948.1863212585449, 302.46907353401184, 1031.769733428955, 430.90051531791687], 'pear')\\npear (7): ([501.7671251296997, 371.20080292224884, 600.0220441818237, 477.94571578502655], 'pear')\\npear (8): ([579.2618751525879, 393.62665593624115, 659.7358512878418, 501.1235100030899], 'pear')\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"\"\"Now, please identifiy any errors or missing labels in the next image. Provided caption: \"\"\"\n",
    "text2 = text2 + context_pear\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8q2phOt1556w4tx7WqU92SYBQLnYW', 'object': 'chat.completion', 'created': 1707414637, 'model': 'gpt-4-1106-vision-preview', 'usage': {'prompt_tokens': 3922, 'completion_tokens': 104, 'total_tokens': 4026}, 'choices': [{'message': {'role': 'assistant', 'content': 'Based on the provided image, here is a list of labels with corrections to the indices and types of fruits they correspond to:\\n\\n- pear (0)\\n- pear (1)\\n- pear (2)\\n- pear (3)\\n- pear (4)\\n- pear (5)\\n- pear (6)\\n- pear (7)\\n- pear (8)\\n\\nThere are no new fruits or vegetables in the image beyond the pears. Each labeled item appears to be a pear, correctly matching the label. No adjustments are needed.'}, 'finish_reason': 'stop', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "# Path to your image\n",
    "image_path = \"data_snapshots/rgb_image_ur5.png\"\n",
    "image_path_orange = 'data_snapshots/owlvit_inference-orange.png'\n",
    "image_path_pear = 'data_snapshots/owlvit_inference-pear.png'\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "base64_image_orange = encode_image(image_path_orange)\n",
    "base64_image_pear = encode_image(image_path_pear)\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# resp1 = \"\"\"\n",
    "# I see: [\"a photo of a lemon\", \"a photo of an apple\", \"a photo of a pear\", \"a photo of an onion\", \"a photo of a lime\"]\n",
    "# Labels: [\"lemon\", \"apple\", \"pear\", \"onion\", \"lime\"]\n",
    "# \"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Please generate a list of labels for this new image of the grocery bin, in the same format.\n",
    "Are there any new fruits or vegetables?\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "No further information required.\n",
    "Now I am looking at a painting of people looking at a shore.\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4-vision-preview\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{text1}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image_orange}\"\n",
    "            }\n",
    "          }\n",
    "          ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{resp1}\"\n",
    "          }]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"{text2}\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          }\n",
    "          ,\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image_pear}\"\n",
    "            }\n",
    "          }\n",
    "          ]\n",
    "      }          \n",
    "      ],\n",
    "    \"max_tokens\": 1000\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided image, here is a list of labels with corrections to the indices and types of fruits they correspond to:\\n\\n- pear (0)\\n- pear (1)\\n- pear (2)\\n- pear (3)\\n- pear (4)\\n- pear (5)\\n- pear (6)\\n- pear (7)\\n- pear (8)\\n\\nThere are no new fruits or vegetables in the image beyond the pears. Each labeled item appears to be a pear, correctly matching the label. No adjustments are needed.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = response.json()[\"choices\"][0]['message']['content']\n",
    "generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
